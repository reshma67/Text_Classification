{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classify_texts.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"GwKYDGND7Tj8","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qjEuctu37qfw","colab_type":"code","colab":{}},"cell_type":"code","source":["categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n","news_group_train = fetch_20newsgroups(subset = 'train', categories = categories)\n","news_group_test = fetch_20newsgroups(subset = 'test', categories = categories) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"W0Ujdey6JI3a","colab_type":"code","colab":{}},"cell_type":"code","source":[" news_group_train.target"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wtZwedYi85Iw","colab_type":"code","colab":{}},"cell_type":"code","source":["len(news_group_train.data),len(news_group_test.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pdb9pW5_9dlJ","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab = Counter()\n","\n","for text in news_group_train.data:\n","  for word in text.split(' '):\n","    vocab[word.lower()] += 1\n","\n","for text in news_group_test.data:\n","  for word in text.split(' '):\n","    vocab[word.lower()] += 1\n","    \n","total_words = len(vocab) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"LNi-jT9H-f_g","colab_type":"code","colab":{}},"cell_type":"code","source":["total_words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NJNtu-fH-j5f","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_word_2_index(vocab):\n","  word2index = {}\n","  for i,word in enumerate(vocab):\n","    word2index[word.lower()] = i\n","  return word2index\n","\n","word2index = get_word_2_index(vocab)\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"LyuDvG_B_eMX","colab_type":"code","colab":{}},"cell_type":"code","source":["len(word2index)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iCQ3GNW5_iRH","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_batches(df, i, batch_size):\n","  batches = []\n","  results = []\n","  \n","  texts = df.data[i*batch_size: i*batch_size+ batch_size]\n","  categories = df.target[i*batch_size:i*batch_size+batch_size]\n","  \n","  for text in texts:\n","    layer = np.zeros(total_words,dtype = float)\n","    for word in text.split(' '):\n","        layer[word2index[word.lower()]] += 1\n","     \n","    batches.append(layer)\n","    \n","  for category in categories:\n","    index_ = -1\n","    if category == 0:\n","      index_y = 0\n","    elif category == 1:\n","      index_y = 1\n","    else:\n","      index_y = 2\n","    results.append(index_y)\n","    \n","  return np.array(batches), np.array(results)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lx9_AFpbDi-l","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","num_epochs = 10\n","batch_size = 150\n","display_step = 1\n","\n","\n","hidden_size = 100\n","input_size = total_words\n","num_classes = 3\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HoqhnxztEY0k","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jpr5d-olEdHJ","colab_type":"code","colab":{}},"cell_type":"code","source":["class OurNet(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_classes):\n","    super(OurNet,self).__init__()\n","    self.layer_1 = nn.Linear(input_size,hidden_size, bias = True)\n","    self.relu = nn.ReLU()\n","    self.layer_2 = nn.Linear(hidden_size,hidden_size, bias = True)\n","    self.output_layer = nn.Linear(hidden_size, num_classes, bias = True)\n","    \n","  def forward(self,x):\n","    out = self.layer_1(x)\n","    out = self.relu(out)\n","    out = self.layer_2(out)\n","    out = self.relu(out)\n","    out = self.output_layer(out)\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RWVsSUPNEeDM","colab_type":"code","colab":{}},"cell_type":"code","source":["net = OurNet(input_size, hidden_size, num_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TouB2cyeEkoe","colab_type":"code","colab":{}},"cell_type":"code","source":["loss = nn.CrossEntropyLoss()\n","input = Variable(torch.randn(2, 5), requires_grad=True)\n","print(\">>> batch of size 2 and 5 possible classes\")\n","print(input)\n","target = Variable(torch.LongTensor(2).random_(5))\n","print(\">>> array of size 'batch_size' with the index of the maxium label for each item\")\n","print(target)\n","output = loss(input, target)\n","output.backward()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gXPm6nsm4DZE","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(num_epochs):\n","    total_batch = int(len(news_group_train.data)/batch_size)\n","    # Loop over all batches\n","    for i in range(total_batch):\n","        batch_x,batch_y = get_batches(news_group_train,i,batch_size)\n","        articles = Variable(torch.FloatTensor(batch_x))\n","        labels = Variable(torch.LongTensor(batch_y))\n","        #print(\"articles\",articles)\n","        #print(batch_x, labels)\n","        #print(\"size labels\",labels.size())\n","        \n","        # Forward + Backward + Optimize\n","        optimizer.zero_grad()  # zero the gradient buffer\n","        outputs = net(articles)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 4 == 0:\n","            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n","                   %(epoch+1, num_epochs, i+1, len(news_group_train.data)//batch_size, loss.data))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5cldtCDYHQ1l","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"wT0dKsixzTIE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Test the Model\n","correct = 0\n","total = 0\n","total_test_data = len(news_group_test.target)\n","batch_x_test,batch_y_test = get_batches(news_group_test,0,total_test_data)\n","articles = Variable(torch.FloatTensor(batch_x_test))\n","labels = torch.LongTensor(batch_y_test)\n","outputs = net(articles)\n","_, predicted = torch.max(outputs.data, 1)\n","total += labels.size(0)\n","correct += (predicted == labels).sum()\n","    \n","print('Accuracy of the network on the 1180 test articles: %d %%' % (100 * correct / total))"],"execution_count":0,"outputs":[]}]}